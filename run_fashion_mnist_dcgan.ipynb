{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.misc import imresize\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Reshape, Flatten, Dense\n",
    "from keras.layers import Activation, LeakyReLU, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  preprocess(x):\n",
    "    return (x/255)*2-1\n",
    "\n",
    "def deprocess(x):\n",
    "    return np.uint8((x+1)/2*255) # make sure to use uint8 type otherwise the image won't display properly \n",
    "\n",
    "def load_minst_data():\n",
    "    # load the data\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "    # normalize our inputs to be in the range[-1, 1]\n",
    "    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
    "    # convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have\n",
    "    # 784 columns per row\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "def make_generator(input_size, leaky_alpha, init_stddev):\n",
    "    # generates images in (28,28,1)\n",
    "    return Sequential([\n",
    "        Dense(7*7*1024, input_shape=(input_size,), \n",
    "              kernel_initializer=RandomNormal(stddev=init_stddev)),\n",
    "        Reshape(target_shape=(7, 7, 1024)),\n",
    "        BatchNormalization(),\n",
    "        Conv2DTranspose(256, kernel_size=5, strides=2, padding='same', \n",
    "                        kernel_initializer=RandomNormal(stddev=init_stddev)), # 14x14\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=leaky_alpha),\n",
    "        Dropout(0.5),\n",
    "        Conv2DTranspose(1, kernel_size=5, strides=2, padding='same', \n",
    "                        kernel_initializer=RandomNormal(stddev=init_stddev)), # 28x28\n",
    "        Activation('tanh')\n",
    "    ])\n",
    "\n",
    "def make_discriminator(leaky_alpha, init_stddev):\n",
    "    # classifies images in (28,28,1)\n",
    "    return Sequential([        \n",
    "        Conv2D(128, kernel_size=5, strides=2, padding='same', \n",
    "               kernel_initializer=RandomNormal(stddev=init_stddev),    # 14x14\n",
    "               input_shape=(28, 28, 1)),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=leaky_alpha),\n",
    "        Dropout(0.5),\n",
    "        Conv2D(128, kernel_size=4, strides=2, padding='same', \n",
    "               kernel_initializer=RandomNormal(stddev=init_stddev)),   # 7x7\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=leaky_alpha),\n",
    "        Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(1, kernel_initializer=RandomNormal(stddev=init_stddev)),\n",
    "        Activation('sigmoid')        \n",
    "    ])\n",
    "\n",
    "# beta_1 is the exponential decay rate for the 1st moment estimates in Adam optimizer\n",
    "def make_DCGAN(sample_size, \n",
    "               g_learning_rate, \n",
    "               g_beta_1,\n",
    "               d_learning_rate,\n",
    "               d_beta_1,\n",
    "               leaky_alpha,\n",
    "               init_std):\n",
    "    # generator\n",
    "    generator = make_generator(sample_size, leaky_alpha, init_std)\n",
    "\n",
    "    # discriminator\n",
    "    discriminator = make_discriminator(leaky_alpha, init_std)\n",
    "    discriminator.compile(optimizer=Adam(lr=d_learning_rate, beta_1=d_beta_1), loss='binary_crossentropy')\n",
    "    \n",
    "    # GAN\n",
    "    gan = Sequential([generator, discriminator])\n",
    "    gan.compile(optimizer=Adam(lr=g_learning_rate, beta_1=g_beta_1), loss='binary_crossentropy')\n",
    "    \n",
    "    return gan, generator, discriminator\n",
    "\n",
    "def make_latent_samples(n_samples, sample_size):\n",
    "    #return np.random.uniform(-1, 1, size=(n_samples, sample_size))\n",
    "    return np.random.normal(loc=0, scale=1, size=(n_samples, sample_size))\n",
    "\n",
    "def make_trainable(model, trainable):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = trainable\n",
    "        \n",
    "def make_labels(size):\n",
    "    return 0 + np.random.rand(size)/10, -np.random.rand(size)/10 + 1\n",
    "\n",
    "def show_losses(losses):\n",
    "    losses = np.array(losses)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(losses.T[0], label='Discriminator')\n",
    "    plt.plot(losses.T[1], label='Generator')\n",
    "    plt.title(\"Validation Losses\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def show_images(generated_images):\n",
    "    n_images = len(generated_images)\n",
    "    cols = 10\n",
    "    rows = n_images//cols\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(n_images):\n",
    "        img = deprocess(generated_images[i])\n",
    "        ax = plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(img.reshape((28,28)))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def train(\n",
    "    g_learning_rate, # learning rate for the generator\n",
    "    g_beta_1,        # the exponential decay rate for the 1st moment estimates in Adam optimizer\n",
    "    d_learning_rate, # learning rate for the discriminator\n",
    "    d_beta_1,        # the exponential decay rate for the 1st moment estimates in Adam optimizer\n",
    "    leaky_alpha,\n",
    "    init_std,\n",
    "    smooth=0,\n",
    "    sample_size=100, # latent sample size (i.e. 100 random numbers)\n",
    "    epochs=3,\n",
    "    batch_size=128,  # train batch size\n",
    "    eval_size=16,    # evaluate size\n",
    "    show_details=True):\n",
    "\n",
    "    # labels for the batch size and the test size\n",
    "    y_train_real, y_train_fake = make_labels(batch_size)\n",
    "    y_eval_real,  y_eval_fake  = make_labels(eval_size)\n",
    "\n",
    "    # create a GAN, a generator and a discriminator\n",
    "    gan, generator, discriminator = make_DCGAN(\n",
    "        sample_size, \n",
    "        g_learning_rate, \n",
    "        g_beta_1,\n",
    "        d_learning_rate,\n",
    "        d_beta_1,\n",
    "        leaky_alpha,\n",
    "        init_std)\n",
    "\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        for i in tqdm(range(len(X_train)//batch_size)):\n",
    "            # real CelebA images\n",
    "            X_batch = X_train[i*batch_size:(i+1)*batch_size]\n",
    "            X_batch_real = X_batch\n",
    "\n",
    "            # latent samples and the generated digit images\n",
    "            latent_samples = make_latent_samples(batch_size, sample_size)\n",
    "            X_batch_fake = generator.predict_on_batch(latent_samples)\n",
    "\n",
    "            # train the discriminator to detect real and fake images\n",
    "            make_trainable(discriminator, True)        \n",
    "            discriminator.train_on_batch(X_batch_real, y_train_real * (1 - smooth))\n",
    "            discriminator.train_on_batch(X_batch_fake, y_train_fake)\n",
    "\n",
    "            # train the generator via GAN\n",
    "            make_trainable(discriminator, False)\n",
    "            gan.train_on_batch(latent_samples, y_train_real)\n",
    "\n",
    "        # evaluate\n",
    "        X_eval = X_test[np.random.choice(len(X_test), eval_size, replace=False)]\n",
    "        X_eval_real = X_eval\n",
    "\n",
    "        latent_samples = make_latent_samples(eval_size, sample_size)\n",
    "        X_eval_fake = generator.predict_on_batch(latent_samples)\n",
    "\n",
    "        d_loss  = discriminator.test_on_batch(X_eval_real, y_eval_real)\n",
    "        d_loss += discriminator.test_on_batch(X_eval_fake, y_eval_fake)\n",
    "        g_loss  = gan.test_on_batch(latent_samples, y_eval_real) # we want the fake to be realistic!\n",
    "\n",
    "        losses.append((d_loss, g_loss))\n",
    "\n",
    "        print(\"Epoch: {:>3}/{} Discriminator Loss: {:>6.4f} Generator Loss: {:>6.4f}\".format(\n",
    "            e+1, epochs, d_loss, g_loss))    \n",
    "        show_images(X_eval_fake[:10])\n",
    "    \n",
    "    # show the result\n",
    "    if show_details:\n",
    "        show_losses(losses)\n",
    "        show_images(generator.predict(make_latent_samples(80, sample_size)))    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train, X_test, y_test) = load_minst_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(g_learning_rate=0.0002, \n",
    "      g_beta_1=0.5, \n",
    "      d_learning_rate=0.0002, \n",
    "      d_beta_1=0.5, \n",
    "      leaky_alpha=0.2, \n",
    "      init_std=0.02,\n",
    "      epochs=80,\n",
    "      batch_size=100);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
